% !TeX spellcheck = en_US
\section{EXPERIMENTS}
\label{sec:experiments}

In this section we present the results obtained with our networks. All the models were trained on a single Nvidia Tesla T4 GPU provided by Google Cloud Platform, on a virtual machine with Tensorflow 1.14.0. \\
We made several experiments with the Yolo algorithm, but we don't provide a detailed list here, since it proved to be unsuitable to solve our task.\\
For the sake of comparing the results of the experiments, we divided the Kaggle training set into a test and validation set (each one is 15\% of the whole dataset) leaving the remaining 70\% of the examples for training. We then split the dataset for the classifier, consisting of all the cropped "ground truth" characters, in the same way. The examples within the test set remain the same through each execution, and the loss and accuracy metrics reported further on are computed with respect to the test set.\\
We list in table \ref{tab:finaltests} the score obtained uploading our submission file on Kaggle, using various model configurations. As a benchmark, we also added the score obtained using the same architecture implemented in the referenced Kaggle notebook. This model used a more complex and slightly deeper architecture for detection and a similar CNN for classification, using a different version of residual block. On the other hand, our detection model is more adherent to the one described in the CenterNet paper. In the table, the \textit{preactivated} classifier is our version, with preactivated residual blocks, while with \textit{kaggle} we identify the version from the kaggle notebook. The same notation apply to the "Detector" column.\\
Submissions to the competition are evaluated on a modified version of the $F_1$ score. To score a true positive, center point coordinates that are within the ground truth bounding box and a matching label must be provided. No other details on the metrics are provided.\\ Note that the term \textit{tiling}, used in the table, refers to the predictions done by dividing each image in 4 tiles, as explained in ยง\ref{sssec:tiling}.

\begin{table*}[h]
	\centering
	\begin{tabular}{llccc}
		\rowcolor[HTML]{EFEFEF} 
		\textbf{Detector} & \textbf{Classifier} & \textbf{Tiling} & \textbf{Augmentation} & \textbf{Kaggle score} \\
		ResNet34          & Preactivated        & Yes             & Yes                   & 0.780                 \\
		ResNet34          & Preactivated        & No              & Yes                   & 0.606                 \\
		ResNet34          & Preactivated        & Yes             & No                    & \textbf{0.797}                      \\
		ResNet50          & Preactivated        & Yes             & Yes                   & 0.723                      \\
		ResNet50          & Preactivated        & No              & Yes                   & 0.491                 \\
		Kaggle            & Kaggle              & Yes             & No                   &  0.772                     \\
		Kaggle            & Kaggle              & No              & No                   & 0.553                     
	\end{tabular}
	\caption{Kaggle score achieved with various configurations.}
	\label{tab:finaltests}
\end{table*}

\subsection{Detector}
\label{ssec:detectorexp}

All models were trained using Adam optimizer with the default Keras hyper-parameters. For the ResNet34 network we used a batch size of $32$ and a learning rate of $1 \cdot 10^{-4}$ for the first 10 epochs, decreasing it to $5 \cdot 10^{-5}$ for the next 50 epochs and using $1 \cdot 10^{-5}$ for the last 70 epochs (130 epochs in total). \\
The same values were used for pre-trained ResNet50 network except for batch size, that we set to 16 because of the deeper architecture, and for learning rate that we further lowered to $5 \cdot 10^{-6}$ after 50 epochs, since the loss function was not converging. We trained it for 95 epochs. Subsequent experiments restarting the training with lower learning rates did not improve the results.\\
We also trained the model from the Kaggle notebook for 138 epochs, but we don't include it in the table since we just used it as a benchmark for the final Kaggle score.\\
The metrics shown in table \ref{tab:expdetector} suggests that, at least with the described configuration, the added complexity of the deeper architecture with pre-trained weights, resulted in worse generalization performances.

\begin{table*}
	\begin{tabular}{lcccccc}
		\rowcolor[HTML]{EFEFEF} 
		\textbf{Detector}   & \textbf{Loss} & \textbf{Heatmap loss} & \textbf{Offset loss} & \textbf{Size loss} & \textbf{IoU (no tiling)} & \textbf{IoU (tiling)} \\
		ResNet34 encoder      & 1.2652        & 0.7887                & 0.3833               & 0.0932             & \textbf{0.5112}                   & \textbf{0.7658}                \\
		ResNet50 encoder      & 1.3792        & 0.8417                & 0.4092               & 0.1284             & 0.4067                   & 0.6760
	\end{tabular}
	\caption{Experiments with two residual architecture for detection. The first one uses ResNet34 as an encoder, the second one uses the deeper ResNet50.}
	\label{tab:expdetector}
\end{table*}

\begin{table*}
	\begin{tabular}{llccccc}
		\rowcolor[HTML]{EFEFEF} 
		\textbf{Detector} & \textbf{Classifier} & \textbf{Augmentation} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Macro avg $\mathbf{F_1}$} & \textbf{Weighted avg $\mathbf{F_1}$} \\
		ResNet34          & Preactivated        & Yes                   & 0.2732        & 0.9413  & \textbf{0.8636} & 0.9396     \\
		ResNet34          & Preactivated        & No                    & \textbf{0.2333}        &  \textbf{0.9492} & 0.6784    & \textbf{0.9512}           \\
		Kaggle          & Kaggle    & No                   & 0.4165           & 0.9214     & 0.5030 & 0.9129         
	\end{tabular}
	\caption{Experiments with data augmentation on classifier.}
	\label{tab:classres}
\end{table*}

\subsection{Classifier}
\label{ssec:classifierexp}

We did three experiments with the classifier, summarized in table \ref{tab:classres}. All models were trained using Adam optimizer and batch size 1024. The first trained for 8 epochs, with a learning rate of $5 \cdot 10^{-4}$ for the first 4 epochs, $1 \cdot 10^{-4}$ for the next 2 epochs, and $5 \cdot 10^{-5}$ for the following ones. The value of the learning rate was decreased manually when we noticed oscillations in the validation loss value. This model has the architecture described in figure \ref{fig:classifier} with preactivated residual blocks and was trained using data augmentation. \\
The second model was trained for 11 epochs with learning rate of $5 \cdot 10^{-4}$ for the first 8 epochs, $1 \cdot 10^{-4}$ for the next 2 epochs, and $5 \cdot 10^{-5}$ for last one. It uses the same architecture of the first one but was trained without data augmentation.
The third model is the one from the Kaggle notebook and was trained for a total of 60 epochs with learning rate $1 \cdot 10^{-5}$ and $1 \cdot 10^{-6}$ after 30 epochs. This model was trained longer, since it showed some small improvements on validation loss. It uses an alternative architecture using standard residual blocks like the one depicted in figure \ref{fig:resblock-id} and was trained without data augmentation.\\
In the table we also include the $F_1$ score. The macro-average $F_1$ score is computed by taking the unweighted average of all per-class $F_1$ scores. On the other hand, the weighted $F_1$ score computes the support-weighted average of this scores.\\
From the results shown, data augmentation does not seem to improve the model ability to generalize.