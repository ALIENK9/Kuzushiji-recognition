% !TeX spellcheck = en_US
\section{RELATED WORK}
\label{sec:relatedwork}

Optical Character Recognition (OCR) of cursive handwriting has proven to be a challenging problem. In particular, character segmentation is recognised as one of the major problems in recognizing unconstrained cursive characters or words. We can roughly divide the character recognition approaches into four categories \cite{Blumenstein2008-we}:

\begin{enumerate}
	\item \textbf{Holistic approaches} (also called global approaches) try to avoid an explicit segmentation process, recognizing entire words without splitting them into single characters. This can be done using an end-to-end neural network and using heuristics based on the number of loops and vertical strokes in each text segment.
	\item \textbf{Classical approaches} perform character segmentation strictly before character classification and recognition, and the two steps are carried out independently. Our method belongs to this category.
	\item \textbf{Recognition-based segmentation approaches} interleaves character segmentation and character classification steps. This can be done by segmenting a word, then classify each character, and if the classifier confidence is not high enough repeat the segmentation step in a different way.
	\item \textbf{Mixed approaches} that combine various techniques belonging to the previous points.
\end{enumerate}

Various text detection algorithms have  been proposed in literature, and we present a selection of articles describing some of these algorithms, summing up the key points and results. \citeauthor{Jo2019-qb}\cite{Jo2019-qb} presented an end-to-end text segmentation approach to separate handwritten text from machine printed documents, like filled out forms and book side annotations. They use a Convolutional Neural Network (CNN) with an encoder-decoder architecture similar to U-Net, using max pooling as downsample operators in the encoder and transposed convolution for upsampling in the decoder. The network is trained end-to-end using Adam optimizer and focal loss, taking in consideration class imbalance. They report a 92.50\% detection accuracy, but state that this was significantly reduced when handwritten text overlapped with printed text. \citeauthor{Blumenstein2008-we}\cite{Blumenstein2008-we} proposes a three steps neural-based segmentation technique. The first step includes an improved heuristic to oversegment the text, that achieved high performances at detecting segmentation points under overlapping strokes. It features a modified vertical histogram analysis more suitable to distinguish between ligatures and holes (empty space inside a letter). The second step involves two neural networks trained with a set of features extracted from the over-segmented words that outputs a confidence score for every segmentation point. The last step consists into fusing the confidence values from the networks to obtain the final word segmentation. The experiments are conducted over a small dataset of cursive english words and resulted in a clear improvement of segmentation score with respect to the standard (non-enhanced) oversegmentation method. \citeauthor{Bluche2016-pt}\cite{Bluche2016-pt} use an end-to-end method to recognise paragraphs of text taken from the IAM database, consisting of images of unconstrained handwritten english text documents. They use a Multi-Dimensional Long Short-Term Memory Recurrent Neural Network (MDLSTM-RCNN) to model an attention based mechanism, effectively reading characters in the right order and learning to skip images, noise and line breaks, emulating the attention mechanism of a human reader. They employ a MDLSTM encoder to produce a feature vector that is passed to the MDLSTM modelling the attention mechanism. The weighted sum of the encoder features and the attention weights are passed to a LSTM layer and to the multilayer perceptron that acts as a decoder. The experiments highlight the power of the proposed attention mechanism to recognise full lines of text, while the performances significantly decrease over single words or very short sentences. Another interesting work with an attention based model applied specifically to Kuzushiji character recognition and translation was presented by \citeauthor{Clanuwat2018-vm}\cite{Clanuwat2018-vm} and featured an encoder-decoder architecture with a CNN for feature extraction and an LSTM decoder with an attention model for generating the target Japanese characters. The encoder CNN is a DenseNet which they claim to outperform ResNet and VGG for some recognition tasks. The experiments have been conducted on multiple tasks, from the translation of single characters, to the translation of entire sentences. Their system outperformed Faster R-CNN and VGG-encoded models based on the reported sentence error rate (SER) metric. Finally, regarding the recognition of single Japanese characters, in \citetitle{School_of_Engineering_Technology_West_Bengal_University_of_Technology_India2014-wb}\cite{School_of_Engineering_Technology_West_Bengal_University_of_Technology_India2014-wb} the authors propose a variation on a previous popular algorithm called STRICT-FB, that allows to extract a set of geometric and topological features that allows to overcome the difficulties in recognizing very similar hiragana characters. One of this features is the Center of Gravity (CoG), obtained from the number of black pixels and their relative positions inside an enclosing circle drawn around the character. The author reported an average recognition rate of 94.1\%, better than the results obtained with the original algorithm, conducted over a set of handwritten character with similar size.